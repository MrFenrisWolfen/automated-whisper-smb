version: "3.8"

services:
  # Whisper GPU
  gpu:
    build:
      context: .            # Verweis auf das Verzeichnis mit dem Dockerfile
      dockerfile: whisper.dockerfile
    container_name: whisper-gpu
    environment:
      WHISPER_MODEL:  "${WHISPER_MODEL:-medium}"
    volumes:
      - data:/app/audio/data   # Gemeinsames Volume mit SMB-Server
      - cache:/root/.cache/whisper  # Persistente Speicherung von Modellen nach dem 1. Download
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]  # GPU nutzen, falls verfügbar
    runtime: nvidia  # Notwendig für GPU-Unterstützung
    profiles:
      - gpu  # Dieser Service wird nur im GPU-Profil gestartet
    restart: unless-stopped

  # Whisper CPU
  cpu:
    build:
      context: .            # Verweis auf das Verzeichnis mit dem Dockerfile
      dockerfile: whisper.dockerfile
    container_name: whisper-cpu
    environment:
      WHISPER_MODEL:  "${WHISPER_MODEL:-small}"
    volumes:
      - data:/app/audio/data
      - cache:/root/.cache/whisper
    profiles:
      - cpu  # Dieser Service wird nur im CPU-Profil gestartet
    restart: unless-stopped

  # SMB-Server
  samba:
    image: dockurr/samba
    container_name: whisper-smb
    environment:
      NAME: "whisper"
      USER: "samba"
      PASS: "secret"
      RW: true
      UID: 1000
      GID: 1000
    ports:
      - "139:139"
      - "445:445"
    volumes:
      - data:/storage
    restart: unless-stopped

# Gemeinsame Volumes
volumes:
  data:
  cache:
